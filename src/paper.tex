\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx, colortbl, makecell}
\usepackage{etoolbox}
\usepackage{hyperref}

\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TSELM: Target speaker extraction using discrete tokens and language models
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Beilong Tang}
\IEEEauthorblockA{\textit{Duke Kunshan University} \\
% \textit{name of organization (of Aff.)}\\
Kunshan, China \\
bt132@duke.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Bang Zeng}
\IEEEauthorblockA{\textit{Duke Kunshan University} \\
% \textit{name of organization (of Aff.)}\\
Kunshan, China \\
bangzeng@whu.edu.cn}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Zhan Jin}
% \IEEEauthorblockA{\textit{Duke Kunshan University} \\
% Kunshan, China \\
% zhan.jin@whu.edu.cn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Ming Li}
\IEEEauthorblockA{\textit{Duke Kunshan University} \\
% \textit{name of organization (of Aff.)}\\
Kunshan, China \\
ming.li369@dukekunshan.edu.cn}
}

\maketitle

\begin{abstract}
We present TSELM, a novel target speaker extraction network using discrete tokens and language models. This model utilizes multiple discretized layers
of WavLM Large as input tokens. It uses cross attention to inject target speaker information and language models to model the sequences. It uses 
a scalable HiFi-GAN to reconstruct the audio from the tokens. It uses 
cross entropy loss to model the probability of output tokens, transforming the complex regression problems into classification problems. To the best of our 
knowledge, we are the first to do target speaker extraction using discrete tokens. However, there is still a gap between our model and continuous methods. 
Our codes and pretrained models are available at: \href{https://github.com/Beilong-Tang/TSELM}{https://github.com/Beilong-Tang/TSELM} .
\end{abstract}

\begin{IEEEkeywords}
target speaker extraction, speech separation, language models, audio discretization
\end{IEEEkeywords}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{assets/model.pdf}
    \caption{Overview of TSELM.}
    \label{model}
    \end{figure*}

    \begin{figure}
        \centering
        \includegraphics[width=0.35\textwidth]{assets/cross_attention.pdf}
        \caption{Details of Cross Attention mechanism.}
        \label{cross_attention}
        \end{figure}


        
\section{Introduction}

In contrast to blind speech separation, which seeks to isolate individual utterances from a mixture of known speakers, target speaker extraction (TSE) focuses on extracting only the voice of the target speaker using auxiliary information specific to that speaker. Current models are predominantly discriminative, employing masking strategies to directly minimize the distance between the estimated and clean speech signals \cite{luo2019conv,spex_plus,sepformer,sef_net}. However, these discriminative approaches often struggle to generalize to unseen data and may introduce undesirable distortions \cite{distortion}. To address these limitations, generative models have been proposed. These models aim to learn the underlying distribution of the target speech and generate it, rather than merely mapping mixed speech to clean speech. Generative models have been shown to achieve performance comparable to that of discriminative models \cite{target_diff,tokensplit} in blind speech separation and TSE.


The discretization of audio has gained significant attention with the advancement of language models (LMs). This method converts audio into discrete tokens, allowing LMs to model them, thereby simplifying audio generation tasks by transforming complex regression problems into classification tasks \cite{dasb}. Currently, there are two main approaches to audio discretization. The first involves the use of neural audio codecs \cite{dac}, which typically capture the acoustic features of the audio \cite{speech_tokenizer}. The second approach leverages self-supervised learning (SSL) models, such as HuBERT \cite{hubert} and WavLM \cite{wavlm}. SSL models have demonstrated outstanding performance across numerous downstream tasks \cite{superb}, as they extract continuous representations rich in semantic and timbral information from speech. As shown in \cite{dasb}, SSL models outperform neural audio codecs in tasks like speech enhancement and separation. Consequently, this paper primarily focuses on exploring the discretization of SSL models.

Discretization methods have been studied for speech 
enhancement \cite{selm} and blind speech separation \cite{dasb,tokensplit}, 
however, this approach has 
been rarely studied for target speaker extraction. In this paper, we present a novel way to do 
target speaker extraction using discrete tokens and LMs (TSELM). Inspired by the blind speech separation 
networks proposed in DASB \cite{dasb}, our model has three stages: encoding, modeling and decoding.
For the encoding stage, reference speech and mixed speech are tokenized using WavLM and 
Kmeans as tokenizer. Unlike SELM \cite{selm}, which uses the 6th layer of WavLM as input to the Kmeans layer, 
we follow the recipes in \cite{dasb} and uses the output from hidden 
layers 1, 3, 7, 12, 18, 23 of WavLM as input to the Kmeans layer. 6 Kmeans models
are trained and applied on each layer output respectively. 
Our results indicate that using multiple layers as 
input is better 
than using only the 6th layer. The reference speech is fed to the encoder directly. 
However, for the mixed speech, instead of directly passing it to the WavLM model, we first concatenated the reference speech on both sides 
of it before passing. After tokenization, we use only the tokens 
corresponding to the mixed speech. For the modeling stage, we utilized an attention 
embedding mechanism to add each embeddings of all layers together. We used a Cross 
Attention mechanism similar in \cite{sef_net} to inject the speaker information. 
An encoder-only LM, following by a linear classifier, is applied after the cross attention module
to output the reconstructed tokens. 
For the decoding stage, we used the pretrained unit HiFi-GAN 
in \cite{unit_hifi} to reconstruct the 
discrete tokens back to audio. Unlike SELM \cite{selm}, where a conformer 
detokenizer is trained 
to reconstruct the WavLM embeddings using Kmeans center embeddings before HiFi-GAN, \cite{unit_hifi} has proposed a scalable HiFi-GAN using dropout mechanism to directly 
reconstruct audio from multiple layers of tokens without a conformer detokenizer. It 
has also eliminated the trouble of training a HiFi-GAN for each layer. The encoder and 
decoder is freezed during the training. The model overview is in Fig.\ref{model}. 
Through extensive studies, we have demonstrated this method has achieved comparable results in terms 
of speech quality and intelligibility. To the best of our knowledge, we are the first to explore 
using discrete tokens to conduct target speaker extraction. Our demos are available at \href{https://beilong-tang.github.io/TSELM.demo/}{https://beilong-tang.github.io/TSELM.demo/}.



\section{Method}
Our model consists of three stages: encoding, modeling, and decoding. The encoder and decoder are 
pretrained models and they are freezed during training. For the encoding stage, we use multiple layers of WavLM 
and publicly available Kmeans 
models in \cite{dasb} to produce the discretized tokens. We use a special concatenation 
strategy to encode the mixture speech. For decoders, we use the scalable HiFi-GAN available in 
\cite{dasb} to reconstruct the audio from discretized tokens. Our main focus is on the
modeling stage. We apply an attention embedding mechanism to embed discrete tokens from different 
layers into one single embedding. Encoder-only LM and linear classifier is applied afterwards to optimize the likelihood of probability 
distribution of the clean tokens. 


\subsection{Encoding}


\begin{table*}
    \caption{The performance of different systems on Libri2Mix testset. For TSELM-L-Hybrid  
    and TSELM, we compare the speaker similarity with the discretized target speech 
    (Target-discrete-WavLM) instead of the target speech due to the observation that 
    the process of discretization already losses the speaker information (0.653 for 
    Target-discrete-WavLM). We use "\_d" to denote it.  }
    \setlength{\tabcolsep}{12pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.3}
    \begin{center}
        \begin{tabular}{ccccccccc}
            \Xhline{2\arrayrulewidth} % Bold top line
            \multirow{2}{*}{System} & \multirow{2}{*}{Category} & \multirow{2}{*}{Model type} & \multirow{2}{*}{Model size} & \multicolumn{3}{c}{ DNSMOS $\uparrow$} & \multirow{2}{*}{dWER $\downarrow$} & \multirow{2}{*}{Spk Sim $\uparrow$} \\
            \cline{5-7}
              &                   &                             &                             & SIG     & BAK     & OVL    &                       &                          \\ 
            \hline
            Mixture                 & - & -                           & -                           & 3.39    & 3.14    & 2.68   & 79.3\%                & -                        \\
            Target-discrete-WavLM   & G & -                          & 330M                           & 3.57    & 4.10    & 3.32   & 11.3\%                & 0.653                    \\
            \hline
            Spex+                   &  D & Continuous                  & 11M                         & 3.36    & 3.76    & 2.98   & 19.3\%                & 0.923                    \\
            \hline
            Continuous-WavLM-L6   & G    & Continuous                  & 220M                        & 3.56    & 4.06    & 3.27   & 14.7\%                & 0.877                    \\
            TSELM-L-Hybrid    & G       & Hybrid                      & 468M                        & 3.50    & 4.06    & 3.22   & 19.8\%                & 0.924\_d                 \\
            TSELM-S-NoConcat           & G       & Discrete                    & 354M                        & 3.47    & 4.03    & 3.19   & 69.6\%                & 0.868\_d                 \\
            \hline
            TSELM-S           & G       & Discrete                    & 354M                        & 3.50    & 4.07    & 3.24   & 28.1\%                & 0.892\_d                 \\
            TSELM-M          & G        & Discrete                    & 389M                        & 3.49    & 4.05    & 3.22   & 28.4\%                & 0.901\_d                 \\
            TSELM-L         & G         & Discrete                    & 468M                        & 3.49    & 4.05    & 3.22   & 27.1\%                & 0.905\_d \\
            \Xhline{2\arrayrulewidth} % Bold top line              
            \end{tabular}
            \linebreak
            \label{main_exp}
      \end{center}
    %   \vspace{-10pt}
  \end{table*}

  \begin{table}
    
    \renewcommand{\arraystretch}{1.2}
    \begin{center}
        \begin{tabular}{ccccccc}
            \Xhline{2\arrayrulewidth} % Bold top line
            \multirow{2}{*}{SSL-Model} & \multirow{2}{*}{Type} & \multicolumn{3}{c}{ DNSMOS $\uparrow$} & \multirow{2}{*}{dWER $\downarrow$} & \multirow{2}{*}{Spk Sim $\uparrow$} \\
            \cline{3-5}
                                                       &                             & SIG     & BAK     & OVL    &                       &                          \\ 
            \hline
            \multirow{2}{*}{HuBERT}                              &  Discrete                          & 3.56    & 4.09    & 3.30   & 81.3\%                & 0.865\_d                        \\
                                             &  Hybrid                          & 3.57    & 4.10    & 3.32   & 35.1\%                & 0.907\_d                        \\
            \hline
            \multirow{2}{*}{WavLM-L6}                              &  Discrete                          & 2.08    & 2.07    & 1.64   & 122.1\%                & 0.589\_d                        \\
            &  Hybrid                          & 3.54    & 3.84    & 3.12   & 30.0\%                & 0.849\_d                        \\
            \hline
            \hline
            \multirow{2}{*}{WavLM}                                     &  Discrete                          & 3.49    & 4.05    & 3.22   & 27.1\%                & 0.905\_d                        \\
                                              &  Hybrid                          & 3.50    & 4.06    & 3.22   & 19.8\%                & 0.924\_d                        \\
            
            \Xhline{2\arrayrulewidth} % Bold top line              
            \end{tabular}
            \linebreak
            \caption{Performance of different SSL models and layer selections. HuBERT model subsitutes the WavLM Large to HuBERT Large as the SSL model.
            The WavLM-L6 uses only the 6th layer of hidden output of WavLM Large. WavLM denotes our TSELM model which uses 6 different output layers. 
            }
            \label{hubert_wavlm_6}
      \end{center}
      \vspace{-25pt}
    \end{table}


We use pretrained SSL model WavLM Large \cite{wavlm} to 
encode the speech into continuous representations. We use the output from 6 hidden 
layers 1, 3, 7, 12, 18, 23. Given a speech signal \(s \in R^{T'} \), the output 
of WavLM will be a tensor \(\bm{r}\) of shape \(n \times T \times E\) where 
\(n\) specifies the number 
of output layers (6 in this case), and  \(T\) is the time 
dimension and \(E\) stands for the 
embedding dimension, specifically 1024 in WavLM Large. The Kmeans tokenizer uses \(n\) numbers of  Kmeans model 
separately on each output layer. Each model uses the same clustering number denoted by \(K\).
After tokenization, the continuous embedding \(\bm{r}\) will be transformed to a tensor \(\bm{d}\) of shape \(n \times T\) where each value \(\bm{d}_{(n_i,t)} \in (0, K-1), n_i \in (0,n), t \in 
(0, T)\). We choose \(K\) to be 1000 in our experiments. For reference speech and mixed speech, we 
use the same Kmeans model and the same layer output of WavLM. The encoder is freezed during training.

The encoding strategy on the mixed speech plays a pivotal role in the model performance. Assume we 
have a reference speech \(s_r \in R^{T^r} \) and mixed speech \(s \in R^{T'}\). 
We follow the 
aforementioned procedure for 
reference speech to get tensor \(\bm{d_r}\) of shape \(n \times T_r\). But for the mixed 
speech, instead of directly following the procedure, we first concatenate it with 
the reference speech to get \(s' = [s_r, s_m, s_r] \in R^{(T^r+T'+T^r)}\). Then we use this speech 
as input for the encoder, obtaining output \(\bm{d'}\) of shape \(n \times (T_r+T+T_r)\) where \(T\) 
denotes the length of output by directly passing the mixed speech without concatenation. \(\bm{d'}\) 
contains the discrete tokens for two pieces of the same reference speech and one piece of the mixed 
speech, we select tensor \(\bm{d}\) belonging only to the mixed speech of shape \(n\times T\) as the 
encoder output. This process is inspired by the training schema of WavLM \cite{wavlm}. To ensure the 
model correctly outputs rich information of the target speaker, it is trained by 
having a clean speech overlapped with another interference less than 50\% of the length of the clean 
speech and using the first utterance as the main speaker. By this way, the model identifiers the 
main speaker and produces embedding focusing on the main speaker. Our later experiments showed that 
this strategy plays an important role in the model performance. 



\subsection{Modeling}
\subsubsection{Attention Embedding}
After obtaining the discrete tensor \(\bm{d}\) with shape \(n\times T\), we use 6
learnable embedding tables each with \(K\) entires to embed the 6 layers 
respectively, each resulting in a tensor of shape \(T \times E\). 
After embedding, we follow the same recipe as in \cite{dasb} to 
aggregate the tensor by using attention mechanism to sum all the 6 tensors. This summation 
keeps the information of each layer while reducing the system complexity by reducing the 
dimension of layers. After attention embedding, we obtain reference embedding \(E_r\) and 
mixture embedding \(E_m\).

\subsubsection{Cross Attention}
One of the key factors in target speaker extraction is to 
inject the information of target speaker 
into the mixture. Spex+ \cite{spex_plus} jointly trains a speaker verification task module 
besides the separator, which might not be optimal for the separator task \cite{sef_net}. SEF-Net \cite{sef_net} has proposed an speaker embedding free way to inject the 
target speaker information into the mixture by using cross attention module, and it has 
achieved superior performance compared to other methods. Therefore, we apply 
the cross attention module to inject the reference embedding into the mixture. The details 
are in Fig.\ref{cross_attention}.
The cross attention module consists of a stack of cross-multiple head attention modules, 
and a FiLM module. 
We use \(E_m\) as query and \(E_r\) as key and value for 
the attention module. The output of the cross-multiple head attention module \(E_{spk}\)
is passed together with \(E_m\) to the feature-wise linear modulation (FiLM) get the final output of the module. The output of 
FiLM is denoted as \(E_f = FiLM(E_m, E_{spk}) = \gamma E_{spk} \cdot E_m  + \beta E_{spk} \) where 
\(\gamma\) and \(\beta\) are learnable parameters denoting the scaling and shifting vectors 
respectively. The output is then fed to the language model for later modeling.  
\subsubsection{Language Modeling}
We use encoder-only language models containing multiple self-attention modules to model the injected 
embedding \(E_f\). Due to encoder-only style, the LM is able to learn from all the positions. 
Finally, 6 linear classifiers each with dimension \(K\) is used to produce the probability 
distribution of each layer respectively.
Cross-entropy loss is applied between 
the output tokens and the clean tokens through discretizing the 
ground truth clean audio. 

\section{Experiments}


  
\subsection{Training}

We use the publicly available Kmeans tokenizer and scalable 
HiFi-GAN decoder in \cite{speechbrain}. The Kmeans tokenizer
is trained on \texttt{train\_clean\_100}, \texttt{train\_clean\_360}, and \texttt{train\_other\_500} of
LibriSpeech \cite{librispeech}, and the scalable HiFi-GAN is trained on \texttt{train\_clean\_100} of 
LibriTTS \cite{libritts}. The modeling stage is trained on 
\texttt{train\_clean\_100} and \texttt{train\_clean\_360} of LibriSpeech. All training data are 
generated on the fly with relative SNR between 0 dB to 5 dB. The mixture audio is segmented to 3 
seconds and the reference speech is segmented to 4 seconds. 

We use the output from hidden layers 1, 3, 7, 12, 18, 23 from WavLM Large and Kmeans model with 
\(K=1000\).
We use embedding dimension 1024,  4 transformer 
encoders each with 16 heads and MLP with hidden dimension of 1024 in the cross attention module. We 
use layer norm after the cross attention module. 
We use separate embedding tables in attention embedding module for reference and mixed speech due to our observations that not 
sharing embeddings does slightly better than sharing embeddings. The LM of small version TSELM-S uses 
embedding dimension \(d\) = 256, absolute sinusoidal positional embedding, conformer encoders as backbone of LM. The conformer encoder is composed of 
6 layers with kernel size 31 each with 4 heads and an MLP with hidden dimension 2048. The medium version TSELM-M uses \(d\) = 512 with 8 layers and 8 heads and the large version TSELM-L uses 
\(d\) = 768 with 12 layers and 16 heads. We use AdamW as 
our optimizer for all the experiments. The learning rate 
is \(5 \times 10^{-4}\) for TSELM-S and \(5 \times 10^{-5}\) for TSELM-M and TSELM-L. We use 8 16Gb ram GPUs. Our batch size is set to 128. We train the model for 40k steps. 


\subsection{Evaluation}
We use the \texttt{dev} set of Libri2Mix \cite{librimix} to test our model performance and compare it with 
other baseline models. 

It is shown that metrics like PESQ, SI-SNR, STOI do not reflect speech quality of output of 
vocoders due to the fact that the vocoder output does not focus strictly on frame alignment
\cite{tokensplit,selm}. We used DNSMOS \cite{dnsmos} to measure the speech quality, and differential 
word error 
rate (dWER) \cite{dwer} to measure the speech intelligibility. For speaker similarity, we use the 
public WeSpeaker \cite{wespeaker}.


\subsection{Baseline models}
All models are reflected in Table \ref{main_exp}. 
TSELM is compared with Spex+, a discriminative separation model \cite{spex_plus} trained on Libri2Mix \cite{librimix}. Since the discretization might lose some 
speaker information compared with continuous methods. We compare the speaker 
similarity of TSELM with the target audio produced by discretizing the clean audio, which is denoted by Target-discrete-WavLM, and it is the upper bound of our model  
performance. Besides TSELM, we did two experiments 
denoted by Continuous-WavLM and TSELM-L-Hybrid using the same training data. 
For Continuous-WavLM, we directly
passed the embeddings of the 6th hidden layer output of WavLM to the cross attention  
and LM. Concatenation strategy is stilled applied to the mixed speech. Mean Square Error loss is applied between the output embeddings and the clean 
embeddings. We use the HiFi-GAN in \cite{knn_vc} to reconstruct the audio. For 
TSELM-L-Hybrid, inspired by MaskSR \cite{mask_sr}, we kept discretizing the reference 
speech while utilizing the continuous embeddings from the reference speech. We call 
it hybrid instead of fully discretized because the mixture speech has continuous 
features instead of discrete features. 

\section{Results and Discussions}
Table \ref{main_exp} shows the performance of different systems on the Libri2Mix test.
The model size for Target-discrete-WavLM denotes the model size of the encoder and 
decoder, which is WavLM Large and HiFi-GAN respectively. DNSMOS is calculated over the 
test sets since it is reference-free. dWER is calculated with the clean speech instead 
of the discretized speech due to our observations that this does not make much difference. 
For TSELM-L-Hybrid and TSELM, we compute the speaker similarity with the discretized
target speech (Target-discrete-WavLM) instead of the target speech.
% due to the observation that the process of discretization already losses the speaker
% information (0.653 for Target-discrete-WavLM). 
We observe that the process of discretization already losses the speaker
information (0.653 for Target-discrete-WavLM), and it might be related to the inevitable speaker 
information loss by the process of discretization. Future work should focus on developing better 
tokenization methods for SSL models. 
Since work focuses on using the discretized information 
to do target speaker extraction instead of developing better tokenization methods, we think it is 
reasonable to compare our outputs with the discretized speech since it serves as an upper bond.   

We observe that our model performs better than Spex+ in terms of the DNSMOS scores but worse in dWER. 
This suggests that our model has better speech quality compared with discriminative models but 
worse in speech intelligibility. One possible reason might be related 
to the discretization process on the mixture 
speech. Our Kmeans algorithm is trained on clean speech instead of mixed speech. 
For speech enhancement, this might be 
good because the level of noise will usually be smaller than the speech. Kmeans might achieve the 
effect of denoising. Discretization on the mixed
speech, however, might cause the output to focus on wrong speakers. 
Our results from TSELM-L-Hybrid 
support 
this. For TSELM-L-Hybrid, we use the continuous embeddings from the mixture speech without discretizing, and it has 
achieved similar dWER with Spex+. Another reason might be related to LM. Right now our encoder-only 
LM satisfies roughly 50\% accuracy. We think using auto-regressive or masking models will achieve 
better results. 

We observe significant dWER increasing without concatenating the reference audio to the mixture, 
as shown 
the TSELM-S-NoConcat in Table \ref{main_exp}. WavLM \cite{wavlm} is demonstrated to have target speaker 
separation capability. The input audio should have the mixture less than 50\% and have the first utterance as target speaker. The output will be a coarse denoised embedding emphasizing the target speaker. 
If the whole input is mixture, we have found that WavLM sometimes will 
extract the wrong speaker. Inspired by SELM's \cite{selm} success in speech denoising. Our pipeline is trying to transform
the target speaker separation problem to speech enhancement problem by using WavLM's denoising capability.

In Table \ref{hubert_wavlm_6}, we have compared the performance of different SSL models and the selection between one layer or multiple layers to discretize. 
Our results have shown that the using HuBERT as SSL model has achieved better DNSMOS scores but with worse dWER compared to our WavLM baseline.
The better DNSMOS scores might be related to the vocoders but it does not reflect the speech intelligibility which is important in speech separation. 
For dWER, this might be because that HuBERT is trained on clean speech and that it might not capture the rich speech information of mixed speeches. 
Our results from WavLM-L6 have shown that for speech separation, using multiple layers to discretize is better than using only one layer. 

Finally, we have observed a performance gap between discrete methods and continuous methods (Continuous-WavLM-L6) on target 
speaker extraction as shown in Table \ref{main_exp}. For Continuous-WavLM-L6,
we use the 6th layer embedding output from WavLM from both the mixed speech and reference speech, and use MSE loss between the output from LM and the clean embeddings. This method 
has the best performance in terms of DNSMOS and dWER. It has a better dWER of 13.6\% compared with TSELM-L. This might be related to the 
information loss of discretization process. We hope future research will work to bridge this gap.   

\section{Conclusion}
In this work, we introduced a novel way using discrete tokens and language models for target speaker extraction.
We use multiple hidden layers of WavLM and Kmeans tokenizers as encoder, cross attention and language model as our separation model, and a scalable HiFi-GAN to reconstruct the 
audio. 
Experiments have 
shown that our model can achieve excellent performance in terms of speech quality, and comparable performance in terms of speech intelligibility and 
speaker similarity. However, we still have a gap between continuous methods especially in speech intelligibility and speaker similarity. Future research should focus on shrinking 
this gap. 








% \section*{Acknowledgment}

% We want to thank for Kunshan Super Computing SCNet for providing the computing resources. 

\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}
