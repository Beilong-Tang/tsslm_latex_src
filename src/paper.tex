\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
%Template version as of 6/27/2024

\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{bm}
\usepackage{multirow}
\usepackage{tabularx, colortbl, makecell}
\usepackage{etoolbox}
\usepackage{hyperref}

\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{TSELM: Target speaker extraction using discrete tokens and language models
}

\author{\IEEEauthorblockN{1\textsuperscript{st} Beilong Tang}
\IEEEauthorblockA{\textit{Duke Kunshan University} \\
% \textit{name of organization (of Aff.)}\\
Kunshan, China \\
bt132@duke.edu}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Bang Zeng}
\IEEEauthorblockA{\textit{Duke Kunshan University} \\
% \textit{name of organization (of Aff.)}\\
Kunshan, China \\
bangzeng@whu.edu.cn}
% \and
% \IEEEauthorblockN{3\textsuperscript{rd} Zhan Jin}
% \IEEEauthorblockA{\textit{Duke Kunshan University} \\
% Kunshan, China \\
% zhan.jin@whu.edu.cn}
\and
\IEEEauthorblockN{3\textsuperscript{rd} Ming Li}
\IEEEauthorblockA{\textit{Duke Kunshan University} \\
% \textit{name of organization (of Aff.)}\\
Kunshan, China \\
ming.li369@dukekunshan.edu.cn}
}

\maketitle

\begin{abstract}
  We propose TSELM, a novel target speaker extraction network that leverages discrete tokens and language models. The model utilizes multiple discretized layers from WavLM Large as input tokens and incorporates cross-attention mechanisms to integrate target speaker information. Language models are employed to capture the sequence dependencies, while a scalable HiFi-GAN is used to reconstruct the audio from the tokens. By applying a cross-entropy loss, TSELM models the probability distribution of output tokens, thus converting the complex regression problem of audio generation into a classification task. To the best of our knowledge, this is the first approach to target speaker extraction using discrete tokens. However, a performance gap remains between our method and continuous approaches. The code and pretrained models are available at: \href{https://github.com/Beilong-Tang/TSELM}{https://github.com/Beilong-Tang/TSELM}.
\end{abstract}

\begin{IEEEkeywords}
target speaker extraction, speech separation, language models, audio discretization, Self-Supervised Models
\end{IEEEkeywords}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{assets/model.pdf}
    \caption{Overview of TSELM.}
    \label{model}
    \end{figure*}

    \begin{figure}
        \centering
        \includegraphics[width=0.35\textwidth]{assets/cross_attention.pdf}
        \caption{Details of Cross Attention mechanism.}
        \label{cross_attention}
        \end{figure}


        
\section{Introduction}

In contrast to blind speech separation, which seeks to isolate individual utterances from a mixture of known speakers, target speaker extraction (TSE) focuses on extracting only the voice of the target speaker using auxiliary information specific to that speaker. Current models are predominantly discriminative, employing masking strategies to directly minimize the distance between the estimated and clean speech signals \cite{luo2019conv,spex_plus,sepformer,sef_net}. However, these discriminative approaches often struggle to generalize to unseen data and may introduce undesirable distortions \cite{distortion}. To address these limitations, generative models have been proposed. These models aim to learn the underlying distribution of the target speech and generate it, rather than merely mapping mixed speech to clean speech. Generative models have been shown to achieve performance comparable to that of discriminative models \cite{target_diff,tokensplit} in blind speech separation and TSE.


The discretization of audio has gained significant attention with the advancement of language models (LMs). This method converts audio into discrete tokens, allowing LMs to model them, thereby simplifying audio generation tasks by transforming complex regression problems into classification tasks \cite{dasb}. Currently, there are two main approaches to audio discretization. The first involves the use of neural audio codecs \cite{dac}, which typically capture the acoustic features of the audio \cite{speech_tokenizer}. The second approach leverages self-supervised learning (SSL) models, such as HuBERT \cite{hubert} and WavLM \cite{wavlm}. SSL models have demonstrated outstanding performance across numerous downstream tasks \cite{superb}, as they extract continuous representations rich in semantic and timbral information from speech. As shown in \cite{dasb}, SSL models outperform neural audio codecs in tasks like speech enhancement and separation. Consequently, this paper primarily focuses on exploring the discretization of SSL models.

Discretization methods have been explored for tasks such as speech enhancement \cite{selm} and blind speech separation \cite{dasb,tokensplit}, but their application to target speaker extraction (TSE) has been limited. In this paper, we present a novel approach to target speaker extraction using discrete tokens and language models, named \textbf{TSELM}. Inspired by the blind speech separation networks proposed in DASB \cite{dasb}, our model is structured into three stages: encoding, modeling, and decoding.

In the encoding stage, both reference and mixed speech are tokenized using WavLM and Kmeans as the tokenizer. Unlike SELM \cite{selm}, which utilizes the 6th layer of WavLM as input to the Kmeans layer, we follow the approach in \cite{dasb} and use outputs from WavLM's hidden layers 1, 3, 7, 12, 18, and 23 as input to six separate Kmeans models, one for each layer. Our results show that using multiple layers as input is superior to using only the 6th layer. The reference speech is passed directly to the encoder, while for mixed speech, we concatenate the reference speech to both sides of the mixture before passing it through the WavLM model. After tokenization, we retain only the tokens corresponding to the mixed speech.
In the modeling stage, an attention embedding mechanism is employed to combine the embeddings from all layers. A cross-attention mechanism, similar to that used in \cite{usef_tes}, is applied to inject speaker-specific information. An encoder-only language model, followed by a linear classifier, is then used to generate the reconstructed tokens.
In the decoding stage, we leverage the pretrained scalable HiFi-GAN from \cite{unit_hifi} to reconstruct the audio from the discrete tokens. Unlike SELM \cite{selm}, where a conformer detokenizer is trained to reconstruct WavLM embeddings before passing them through HiFi-GAN, the scalable HiFi-GAN in \cite{unit_hifi} uses a dropout mechanism to directly reconstruct audio from multiple layers of tokens, eliminating the need for a conformer detokenizer and the complexity of training a separate HiFi-GAN for each layer. Both the encoder and decoder are kept frozen during training, and an overview of the model is shown in Fig.\ref{model}.
Extensive experiments demonstrate that our method achieves excellent speech quality and comparable intelligibility results. To the best of our knowledge, this is the first study to explore target speaker extraction using discrete tokens. Our demos are available at \href{https://beilong-tang.github.io/TSELM.demo/}{https://beilong-tang.github.io/TSELM.demo/}.



\section{Method}
% Our model consists of three stages: encoding, modeling, and decoding. The encoder and decoder are 
% pretrained models and they are freezed during training. For the encoding stage, we use hidden layers 1, 3, 7, 12, 18, and 23 (denoted as \(n_l\)) of WavLM-Large 
% and publicly available Kmeans 
% models in \cite{dasb} to produce the discretized tokens. We use a concatenation 
% strategy to encode the mixture speech. For decoders, we use the scalable HiFi-GAN available in 
% \cite{dasb} to reconstruct the audio from discretized tokens. Our main focus is on the
% modeling stage. We apply an attention embedding mechanism to embed discrete tokens from different 
% layers into one single embedding. Encoder-only LM and linear classifier is applied afterwards to optimize the likelihood of probability 
% distribution of the clean tokens. 


\subsection{Encoding}


\begin{table*}
    \caption{The performance of different systems on Libri2Mix testset. For TSELM-L-Hybrid  
    and TSELM, we compare the speaker similarity with the discretized target speech 
    (Target-discrete-WavLM) instead of the target speech due to the observation that 
    the process of discretization already losses the speaker information (0.653 for 
    Target-discrete-WavLM). We use "\_d" to denote it.  }
    \setlength{\tabcolsep}{12pt} % Adjust column spacing
    \renewcommand{\arraystretch}{1.3}
    \begin{center}
        \begin{tabular}{ccccccccc}
            \Xhline{2\arrayrulewidth} % Bold top line
            \multirow{2}{*}{System} & \multirow{2}{*}{Category} & \multirow{2}{*}{Model type} & \multirow{2}{*}{Model size} & \multicolumn{3}{c}{ DNSMOS $\uparrow$} & \multirow{2}{*}{dWER $\downarrow$} & \multirow{2}{*}{Spk Sim $\uparrow$} \\
            \cline{5-7}
              &                   &                             &                             & SIG     & BAK     & OVL    &                       &                          \\ 
            \hline
            Mixture                 & - & -                           & -                           & 3.39    & 3.14    & 2.68   & 79.3\%                & -                        \\
            Target-discrete-WavLM   & G & -                          & 330M                           & 3.57    & 4.10    & 3.32   & 11.3\%                & 0.653                    \\
            \hline
            Spex+                   &  D & Continuous                  & 11M                         & 3.36    & 3.76    & 2.98   & 19.3\%                & 0.923                    \\
            \hline
            Continuous-WavLM-L6   & G    & Continuous                  & 220M                        & 3.56    & 4.06    & 3.27   & 14.7\%                & 0.877                    \\
            TSELM-L-Hybrid    & G       & Hybrid                      & 468M                        & 3.50    & 4.06    & 3.22   & 19.8\%                & 0.924\_d                 \\
            TSELM-S-NoConcat           & G       & Discrete                    & 354M                        & 3.47    & 4.03    & 3.19   & 69.6\%                & 0.868\_d                 \\
            \hline
            TSELM-S           & G       & Discrete                    & 354M                        & 3.50    & 4.07    & 3.24   & 28.1\%                & 0.892\_d                 \\
            TSELM-M          & G        & Discrete                    & 389M                        & 3.49    & 4.05    & 3.22   & 28.4\%                & 0.901\_d                 \\
            TSELM-L         & G         & Discrete                    & 468M                        & 3.49    & 4.05    & 3.22   & 27.1\%                & 0.905\_d \\
            \Xhline{2\arrayrulewidth} % Bold top line              
            \end{tabular}
            \linebreak
            \label{main_exp}
      \end{center}
    %   \vspace{-10pt}
  \end{table*}

  \begin{table}
    
    \renewcommand{\arraystretch}{1.2}
    \begin{center}
        \begin{tabular}{ccccccc}
            \Xhline{2\arrayrulewidth} % Bold top line
            \multirow{2}{*}{SSL-Model} & \multirow{2}{*}{Type} & \multicolumn{3}{c}{ DNSMOS $\uparrow$} & \multirow{2}{*}{dWER $\downarrow$} & \multirow{2}{*}{Spk Sim $\uparrow$} \\
            \cline{3-5}
                                                       &                             & SIG     & BAK     & OVL    &                       &                          \\ 
            \hline
            \multirow{2}{*}{HuBERT}                              &  Discrete                          & 3.56    & 4.09    & 3.30   & 81.3\%                & 0.865\_d                        \\
                                             &  Hybrid                          & 3.57    & 4.10    & 3.32   & 35.1\%                & 0.907\_d                        \\
            \hline
            \multirow{2}{*}{WavLM-L6}                              &  Discrete                          & 2.08    & 2.07    & 1.64   & 122.1\%                & 0.589\_d                        \\
            &  Hybrid                          & 3.54    & 3.84    & 3.12   & 30.0\%                & 0.849\_d                        \\
            \hline
            \hline
            \multirow{2}{*}{WavLM}                                     &  Discrete                          & 3.49    & 4.05    & 3.22   & 27.1\%                & 0.905\_d                        \\
                                              &  Hybrid                          & 3.50    & 4.06    & 3.22   & 19.8\%                & 0.924\_d                        \\
            
            \Xhline{2\arrayrulewidth} % Bold top line              
            \end{tabular}
            \linebreak
            \caption{Performance of different SSL models and layer selections. HuBERT model subsitutes the WavLM Large to HuBERT Large as the SSL model.
            The WavLM-L6 uses only the 6th layer of hidden output of WavLM Large. WavLM denotes our TSELM model which uses 6 different output layers. 
            }
            \label{hubert_wavlm_6}
      \end{center}
      \vspace{-25pt}
    \end{table}


We use the pretrained self-supervised learning (SSL) model WavLM Large \cite{wavlm} to encode speech into continuous representations. Specifically, we extract the outputs from six hidden layers: 1, 3, 7, 12, 18, and 23. Given a speech signal \(s \in \mathbb{R}^{T'}\), the output of WavLM is a tensor \(\bm{r}\) with shape \(n \times T \times E\), where \(n\) is the number of output layers (6 in this case), \(T\) is the time dimension, and \(E\) represents the embedding dimension, which is 1024 in WavLM Large.  For tokenization, we apply separate Kmeans models to each output layer, with each model using the same number of clusters, denoted by \(K\). After tokenization, the continuous embedding \(\bm{r}\) is transformed into a discrete tensor \(\bm{d}\) with shape \(n \times T\), where each value \(\bm{d}_{i} \in (0, K-1) \). In all our experiments, we set \(K = 1000\). For both reference and mixed speech, the same Kmeans model and the same layer combination from WavLM Large are used. The encoder remains frozen during training.
The encoding strategy for mixed speech is crucial to the performance of the model. Given a reference speech \(s_r \in \mathbb{R}^{T^r}\) and a mixed speech \(s_m \in \mathbb{R}^{T'}\), we follow the previously described procedure to encode the reference speech into a tensor \(\bm{d_r}\) of shape \(n \times T_r\). However, for mixed speech, instead of applying the encoding directly, we first concatenate it with the reference speech, creating a signal \(s' = [s_r, s_m, s_r] \in \mathbb{R}^{(T^r + T' + T^r)}\). This concatenated signal is then input into the encoder, producing an output tensor \(\bm{d'}\) with shape \(n \times (T_r + T + T_r)\), where \(T\) represents the time dimension for the mixed speech without concatenation. The tensor \(\bm{d'}\) contains discrete tokens for the two segments of reference speech and the mixed speech. We extract the portion \(\bm{d}\) corresponding only to the mixed speech, resulting in an output tensor of shape \(n \times T\).
This approach is inspired by the training schema of WavLM \cite{wavlm}, which trains the model by overlapping clean speech with an interfering signal that covers less than 50\% of the clean speech's duration, using the first utterance as the primary speaker. This allows WavLM to identify and focus on the main speaker when producing embeddings. Our experiments demonstrated that this concatenation strategy significantly enhances the model's performance by guiding it to prioritize the target speaker's information.
\subsection{Modeling}
\subsubsection{Attention Embedding}
After obtaining the discrete tensor \(\bm{d}\) with shape \(6 \times T\), we use 6
learnable embedding tables each with \(K\) entires to embed the 6 layers 
respectively, each resulting in a tensor of shape \(T \times E\). 
After embedding, we follow the same recipe as in \cite{dasb} to 
aggregate the tensor by using attention mechanism to sum all the 6 tensors. This summation 
keeps the information of each layer while reducing the system complexity by reducing the 
dimension of layers. After attention embedding, we obtain reference embedding \(E_r\) and 
mixture embedding \(E_m\).

\subsubsection{Cross Attention}
One of the key factors in target speaker extraction is to 
inject the information of target speaker 
into the mixture. Spex+ \cite{spex_plus} jointly trains a speaker verification task module 
besides the separator, which might not be optimal for the separator task \cite{sef_net}. SEF-Net \cite{sef_net} has proposed an speaker embedding free way to inject the 
target speaker information into the mixture by using cross attention module, and it has 
achieved superior performance compared to other methods. Therefore, we apply 
the cross attention module to inject the reference embedding into the mixture. The details 
are in Fig.\ref{cross_attention}.
The cross attention module consists of a stack of cross-multiple head attention modules, 
and a FiLM module. 
We use \(E_m\) as query and \(E_r\) as key and value for 
the attention module. The output of the cross-multiple head attention module \(E_{spk}\)
is passed together with \(E_m\) to the feature-wise linear modulation (FiLM) get the final output of the module. The output of 
FiLM is denoted as \(E_f = FiLM(E_m, E_{spk}) = \gamma E_{spk} \cdot E_m  + \beta E_{spk} \) where 
\(\gamma\) and \(\beta\) are learnable parameters denoting the scaling and shifting vectors 
respectively. The output is then fed to the language model for later modeling.  
\subsubsection{Language Modeling}
We use encoder-only language models containing multiple self-attention modules to model the injected 
embedding \(E_f\). Due to encoder-only style, the LM is able to learn from all the positions. 
Finally, 6 linear classifiers each with dimension \(K\) is used to produce the probability 
distribution of each layer respectively.
Cross-entropy loss is applied between 
the output tokens and the clean tokens through discretizing the 
ground truth clean audio. 

\section{Experiments}


  
\subsection{Training}

We use the publicly available Kmeans tokenizer and scalable 
HiFi-GAN decoder in \cite{speechbrain}. The Kmeans tokenizer
is trained on \texttt{train\_clean\_100}, \texttt{train\_clean\_360}, and \texttt{train\_other\_500} of
LibriSpeech \cite{librispeech}, and the scalable HiFi-GAN is trained on \texttt{train\_clean\_100} of 
LibriTTS \cite{libritts}. The modeling stage is trained on 
\texttt{train\_clean\_100} and \texttt{train\_clean\_360} of LibriSpeech. All training data are 
generated on the fly with relative SNR between 0 dB to 5 dB. The mixture audio is segmented to 3 
seconds and the reference speech is segmented to 4 seconds. 

We use the output from hidden layers 1, 3, 7, 12, 18, 23 from WavLM Large and Kmeans model with 
\(K=1000\).
We use embedding dimension 1024,  4 transformer 
encoders each with 16 heads and MLP with hidden dimension of 1024 in the cross attention module. We 
use layer norm after the cross attention module. 
We use separate embedding tables in attention embedding module for reference and mixed speech due to our observations that not 
sharing embeddings does slightly better than sharing embeddings. The LM of small version TSELM-S uses 
embedding dimension \(d\) = 256, absolute sinusoidal positional embedding, conformer encoders as backbone of LM. The conformer encoder is composed of 
6 layers with kernel size 31 each with 4 heads and an MLP with hidden dimension 2048. The medium version TSELM-M uses \(d\) = 512 with 8 layers and 8 heads and the large version TSELM-L uses 
\(d\) = 768 with 12 layers and 16 heads. We use AdamW as 
our optimizer for all the experiments. The learning rate 
is \(5 \times 10^{-4}\) for TSELM-S and \(5 \times 10^{-5}\) for TSELM-M and TSELM-L. We use 8 16Gb ram GPUs. Our batch size is set to 128. We train the model for 40k steps. 


\subsection{Evaluation}
We use the \texttt{dev} set of Libri2Mix \cite{librimix} to test our model performance and compare it with 
other baseline models. 

It is shown that metrics like PESQ, SI-SNR, STOI do not reflect speech quality of output of 
vocoders due to the fact that the vocoder output does not focus strictly on frame alignment
\cite{tokensplit,selm}. We used DNSMOS \cite{dnsmos} to measure the speech quality, and differential 
word error 
rate (dWER) \cite{dwer} to measure the speech intelligibility. For speaker similarity, we use the 
public WeSpeaker \cite{wespeaker}.


\subsection{Baseline models}
All models are reflected in Table \ref{main_exp}. 
TSELM is compared with Spex+, a discriminative separation model \cite{spex_plus} trained on Libri2Mix \cite{librimix}. Since the discretization might lose some 
speaker information compared with continuous methods. We compare the speaker 
similarity of TSELM with the target audio produced by discretizing the clean audio, which is denoted by Target-discrete-WavLM, and it is the upper bound of our model  
performance. Besides TSELM, we did two experiments 
denoted by Continuous-WavLM and TSELM-L-Hybrid using the same training data. 
For Continuous-WavLM, we directly
passed the embeddings of the 6th hidden layer output of WavLM to the cross attention  
and LM. Concatenation strategy is stilled applied to the mixed speech. Mean Square Error loss is applied between the output embeddings and the clean 
embeddings. We use the HiFi-GAN in \cite{knn_vc} to reconstruct the audio. For 
TSELM-L-Hybrid, inspired by MaskSR \cite{mask_sr}, we kept discretizing the reference 
speech while utilizing the continuous embeddings from the reference speech. We call 
it hybrid instead of fully discretized because the mixture speech has continuous 
features instead of discrete features. 

\section{Results and Discussions}
Table \ref{main_exp} shows the performance of different systems on the Libri2Mix test.
The model size for Target-discrete-WavLM denotes the model size of the encoder and 
decoder, which is WavLM Large and HiFi-GAN respectively. DNSMOS is calculated over the 
test sets since it is reference-free. dWER is calculated with the clean speech instead 
of the discretized speech due to our observations that this does not make much difference. 
For TSELM-L-Hybrid and TSELM, we compute the speaker similarity with the discretized
target speech (Target-discrete-WavLM) instead of the target speech.
% due to the observation that the process of discretization already losses the speaker
% information (0.653 for Target-discrete-WavLM). 
We observe that the process of discretization already losses the speaker
information (0.653 for Target-discrete-WavLM), and it might be related to the inevitable speaker 
information loss by the process of discretization. Future work should focus on developing better 
tokenization methods for SSL models. 
Since work focuses on using the discretized information 
to do target speaker extraction instead of developing better tokenization methods, we think it is 
reasonable to compare our outputs with the discretized speech since it serves as an upper bond.   

We observe that our model performs better than Spex+ in terms of the DNSMOS scores but worse in dWER. 
This suggests that our model has better speech quality compared with discriminative models but 
worse in speech intelligibility. One possible reason might be related 
to the discretization process on the mixture 
speech. Our Kmeans algorithm is trained on clean speech instead of mixed speech. 
For speech enhancement, this might be 
good because the level of noise will usually be smaller than the speech. Kmeans might achieve the 
effect of denoising. Discretization on the mixed
speech, however, might cause the output to focus on wrong speakers. 
Our results from TSELM-L-Hybrid 
support 
this. For TSELM-L-Hybrid, we use the continuous embeddings from the mixture speech without discretizing, and it has 
achieved similar dWER with Spex+. Another reason might be related to LM. Right now our encoder-only 
LM satisfies roughly 50\% accuracy. We think using auto-regressive or masking models will achieve 
better results. 

We observe significant dWER increasing without concatenating the reference audio to the mixture, 
as shown 
the TSELM-S-NoConcat in Table \ref{main_exp}. WavLM \cite{wavlm} is demonstrated to have target speaker 
separation capability. The input audio should have the mixture less than 50\% and have the first utterance as target speaker. The output will be a coarse denoised embedding emphasizing the target speaker. 
If the whole input is mixture, we have found that WavLM sometimes will 
extract the wrong speaker. Inspired by SELM's \cite{selm} success in speech denoising. Our pipeline is trying to transform
the target speaker separation problem to speech enhancement problem by using WavLM's denoising capability.

In Table \ref{hubert_wavlm_6}, we have compared the performance of different SSL models and the selection between one layer or multiple layers to discretize. 
Our results have shown that the using HuBERT as SSL model has achieved better DNSMOS scores but with worse dWER compared to our WavLM baseline.
The better DNSMOS scores might be related to the vocoders but it does not reflect the speech intelligibility which is important in speech separation. 
For dWER, this might be because that HuBERT is trained on clean speech and that it might not capture the rich speech information of mixed speeches. 
Our results from WavLM-L6 have shown that for speech separation, using multiple layers to discretize is better than using only one layer. 

Finally, we have observed a performance gap between discrete methods and continuous methods (Continuous-WavLM-L6) on target 
speaker extraction as shown in Table \ref{main_exp}. For Continuous-WavLM-L6,
we use the 6th layer embedding output from WavLM from both the mixed speech and reference speech, and use MSE loss between the output from LM and the clean embeddings. This method 
has the best performance in terms of DNSMOS and dWER. It has a better dWER of 13.6\% compared with TSELM-L. This might be related to the 
information loss of discretization process. We hope future research will work to bridge this gap.   

\section{Conclusion}
In this work, we introduced a novel way using discrete tokens and language models for target speaker extraction.
We use multiple hidden layers of WavLM and Kmeans tokenizers as encoder, cross attention and language model as our separation model, and a scalable HiFi-GAN to reconstruct the 
audio. 
Experiments have 
shown that our model can achieve excellent performance in terms of speech quality, and comparable performance in terms of speech intelligibility and 
speaker similarity. However, we still have a gap between continuous methods especially in speech intelligibility and speaker similarity. Future research should focus on shrinking 
this gap. 








% \section*{Acknowledgment}

% We want to thank for Kunshan Super Computing SCNet for providing the computing resources. 

\bibliographystyle{IEEEtran}
\bibliography{references}


\end{document}
